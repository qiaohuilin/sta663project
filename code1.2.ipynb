{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import h5py\n",
    "warnings.resetwarnings()\n",
    "warnings.simplefilter(action='ignore', category=ImportWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=ResourceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape and standardize\n",
    "x=train_data\n",
    "y=test_data\n",
    "xres=x.reshape(60000,-1)\n",
    "yres=y.reshape(10000,-1)\n",
    "xsd=(sklearn.preprocessing.normalize(xres,axis=0))\n",
    "ysd=(sklearn.preprocessing.normalize(yres,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=60000\n",
    "M=100\n",
    "alpha=0.01 #step size,        or 0.02, 0.1\n",
    "D=28*28 #dimension of x_i 28*28\n",
    "J=2   #dimension of z         or 5,10,20\n",
    "\n",
    "#initialize \\phi\n",
    "u=np.zeros([10000,J])  #assume 10000 iterations, can kick out empty rows if converge early\n",
    "sig2=np.zeros([10000,J])\n",
    "u[0,]=np.random.normal(0,0.01,J)\n",
    "sig2[0,]=np.random.normal(0,0.01,J)**2\n",
    "#initialize \\theta\n",
    "uprime=np.zeros([10000,D])  #assume 10000 iterations, can kick out empty rows if converge early\n",
    "sig2prime=np.zeros([10000,D])\n",
    "uprime[0,]=np.random.normal(0,0.01,D)\n",
    "sig2prime[0,]=np.random.normal(0,0.01,D)**2\n",
    "\n",
    "#set seed\n",
    "np.random.seed(123)\n",
    "L=np.zeros([10000,M])        # we want to keep the track of lower bound each iter to replicate Fig2\n",
    "LM=np.zeros(10000)           #LM with batch size M, for 10000 iter\n",
    "#maybe we can get rid of Ls if the code is slow and only keep the SGVB update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    t=0\n",
    "    xm=xsd[np.random.randint(0,N-1,M),]\n",
    "    u_temp=np.random.normal(0,0.01,[M,J])\n",
    "    sig2_temp=np.random.normal(0,0.01,[M,J])**2\n",
    "    uprime_temp=np.random.normal(0,0.01,[M,D])\n",
    "    sig2prime_temp=np.random.normal(0,0.01,[M,D])**2\n",
    "    \n",
    "    for i in range(M):\n",
    "        # MLP use x_i to get \\phi_t(i) u_temp[i,] and sig2_temp[i,], which is same dimension as z (J)\n",
    "        eps=np.random.multivariate_normal(np.zeros(J),np.eye(2))\n",
    "        z=u_temp[i,]+sig2_temp[i,]*eps\n",
    "        # MLP use z_i to get \\theta_t(i) uprime_temp[i,] and sig2prime_temp[i,], which is same dimension as x(D)\n",
    "        L[t,i]=1/2 * (1+sum(np.log(sig2_temp[i,]))-sum(u_temp[i,])-sum(sig2_temp[i,])) - 1/2* (D*np.log(2*np.pi)+sum(np.log(sig2prime_temp[i,])))- 1/2 * sum((xm[i,]-uprime_temp[i,])**2*1/sig2prime_temp[i,])\n",
    "        LM[t]=N/M*sum(L[t,])\n",
    "        \n",
    "    for j in range(J):\n",
    "        u[t+1,j]=u[t,j]- alpha*N/M*(-sum(u_temp[:,j]))    # to confirm it's minus?? gradient descent. the written part says ascent\n",
    "        sig2[t+1,j]=sig2[t,j] -alpha*N/M*(1/2)*sum(1/sig2_temp[:,j]-1)\n",
    "    \n",
    "    for d in range(D):\n",
    "        uprime[t+1,d]=uprime[t,d]-alpha*N/M*sum(1/sig2prime_temp[:,d]*(xm[:,d]-uprime_temp[:,d]))\n",
    "        sig2prime[t+1,d]=sig2prime[t,d]-alpha*N/M*sum(1/2 * (1/(sig2prime_temp[:,d])**2*(xm[:,d]-uprime_temp[:,d])**2-1/(sig2prime_temp[:,d])))\n",
    "        \n",
    "    if (np.norm(u[t+1,]-u[t,])<1e-4) & (np.norm(sig2[t+1,]-sig2[t,])<1e-4) & (np.norm(uprime[t+1,]-uprime[t,])<1e-4) & (np.norm(sig2prime[t+1,]-sig2prime[t,])<1e-4):\n",
    "        break\n",
    "    \n",
    "    t=t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-233411.4310329574"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/2 * (1+sum(np.log(sig2_temp[i,]))-sum(u_temp[i,])-sum(sig2_temp[i,])) - 1/2* (D*np.log(2*np.pi)+sum(np.log(sig2prime_temp[i,])))- 1/2 * sum((xm[i,]-uprime_temp[i,])**2*1/sig2prime_temp[i,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4609140766449149"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha*N/M*(-sum(u_temp[:,j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5259360311174.01"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha*N/M*(1/2)*sum(1/sig2_temp[:,j]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12835891.164692566"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha*N/M*sum(1/sig2prime_temp[:,d]*(xm[:,d]-uprime_temp[:,d]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14716640191595.527"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha*N/M*sum(1/2 * (1/(sig2prime_temp[:,d])**2*(xm[:,d]-uprime_temp[:,d])**2-1/(sig2prime_temp[:,d])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
